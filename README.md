# IAM Policy Classification Engine ğŸ”ğŸ¤–

A research-oriented system for **automatic security classification of AWS IAM policies** using Large Language Models (LLMs).  
The project evaluates how well modern LLMs can reason about IAM permissions, least-privilege principles, and conditional access controls.

---

## âœ¨ Key Features

- âœ… Accepts **AWS IAM policies in JSON format**
- âœ… Classifies policies as **Strong** or **Weak**
- âœ… Produces **concise, security-focused reasoning**
- âœ… Supports **multiple LLM providers** (Hugging Face & OpenAI)
- âœ… Enforces **structured JSON output** via schema validation
- âœ… Includes an **evaluation pipeline** with accuracy metrics
- âœ… Designed for **analysis and research**, not production enforcement

---

## ğŸ§  Motivation

IAM misconfigurations are a leading cause of cloud security incidents.  
This project explores whether Large Language Models can reliably analyze IAM policies and distinguish secure configurations from risky ones - including **borderline cases** that require semantic reasoning rather than simple pattern matching.

---

## ğŸ—‚ Project Structure

```
IAM-Policy-Classification-Engine/
â”‚
â”œâ”€â”€ main.py                     # Entry point - runs policy classification
â”œâ”€â”€ evaluate.py                 # Evaluation script over labeled datasets
â”‚
â”œâ”€â”€ classifier.py               # Core classification logic
â”œâ”€â”€ llm_client.py               # Generic LLM client interface
â”œâ”€â”€ prompt.py                   # Prompt templates and construction
â”œâ”€â”€ schemas.py                  # Output JSON schemas
â”œâ”€â”€ schema_self_check.py        # Self-check logic for the scheme
â”‚
â”œâ”€â”€ providers.py                # Provider abstraction
â”œâ”€â”€ openai_provider.py          # OpenAI-specific implementation
â”œâ”€â”€ huggingface_provider.py     # Hugging Face-specific implementation
â”‚
â”œâ”€â”€ logging_utils.py            # Centralized logging utilities
â”‚
â”œâ”€â”€ policies/                   # Example IAM policies (JSON)
â”‚   â”œâ”€â”€ strong_policy.json
â”‚   â”œâ”€â”€ weak_policy.json
â”‚
â”œâ”€â”€ outputs/                    # Generated outputs and evaluation results
â”‚   â””â”€â”€ evaluation_results.json
â”‚
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md

```

---

## âš™ï¸ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/guybaruch1/IAM-Policy-Classification-Engine.git
cd IAM-Policy-Classification-Engine
```

### 2. Create a Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate
# Windows:
.venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Configuration

Create a `.env` file based on `.env.example`:

```env
HF_API_TOKEN=your_huggingface_token
HF_MODEL=meta-llama/Meta-Llama-3-8B-Instruct

OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-3.5-turbo
```

---

## ğŸš€ Usage

### ğŸ”¹ Classify a Single IAM Policy

```bash
python main.py policies/weak_policy.json
```

### ğŸ”¹ Evaluate Multiple Policies

```bash
python evaluate.py
```

Results are saved to:
```
outputs/evaluation_results.json
```

---

## ğŸ§ª Evaluation Summary

- Dataset size: 12 IAM policies
- Includes strong, weak, and borderline cases
- Accuracy comparison across LLMs
- Structured JSON outputs validated via schema

---

## âš ï¸ Limitations

- Research-oriented (not production-ready)
- Dependent on LLM reasoning quality
- IAM policies can be semantically ambiguous

---

## ğŸ“Œ Future Work

- Larger datasets
- Confusion matrices
- Additional LLM providers
- Hybrid rule-based + LLM analysis

---

## ğŸ“„ License

Academic & research use only.
